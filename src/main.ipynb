{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waste classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import tarfile\n",
    "\n",
    "# Numerical & Data Processing\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning & Data Splitting\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Deep Learning & PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_DIR = os.path.join(\"..\", \"res\")\n",
    "DATASET_DIR = os.path.join(RES_DIR, \"dataset\")\n",
    "OUTPUT_DIR = os.path.join(RES_DIR, \"output\")\n",
    "\n",
    "TRAIN_DIR = os.path.join(OUTPUT_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(OUTPUT_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(OUTPUT_DIR, \"test\")\n",
    "MODEL_DIR = os.path.join(OUTPUT_DIR, \"model\")\n",
    "\n",
    "MODEL_NAME = \"model.pt\"\n",
    "\n",
    "CLASSES = os.listdir(DATASET_DIR)\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "SEED = 42\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "\n",
    "SHOW_VISUALIZATIONS = True  # Avoid visualization diagrams\n",
    "TESTING = False  # Avoid time consuming operations (K-Fold, model training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the paths and labels all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "labels = []\n",
    "\n",
    "for idx, label in enumerate(os.listdir(DATASET_DIR)):\n",
    "    class_dir = os.path.join(DATASET_DIR, label)\n",
    "    for video in os.listdir(class_dir):\n",
    "        image_path = os.path.join(class_dir, video)\n",
    "\n",
    "        paths.append(image_path)\n",
    "        labels.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_VISUALIZATIONS:\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(x=unique, y=counts, palette=\"viridis\", hue=unique, legend=False)\n",
    "    plt.title(\"Class Distribution\")\n",
    "\n",
    "    plt.xticks(ticks=np.arange(NUM_CLASSES), labels=CLASSES)\n",
    "\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "\n",
    "    for i, count in enumerate(counts):\n",
    "        ax.text(i, count + 1, str(count), ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Visualize a sample of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_VISUALIZATIONS:\n",
    "    num_images = 12\n",
    "    columns = 4\n",
    "    rows = (num_images + columns - 1) // columns\n",
    "\n",
    "    plt.figure(figsize=(columns * 3, rows * 3))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Get random class\n",
    "        class_name = random.choice(CLASSES)\n",
    "        class_path = os.path.join(DATASET_DIR, class_name)\n",
    "\n",
    "        # Get random image from that class\n",
    "        image_name = random.choice(os.listdir(class_path))\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "\n",
    "        # Load and display image\n",
    "        img = Image.open(image_path)\n",
    "        plt.subplot(rows, columns, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(class_name.title())\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create directories for the train, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the dataset into a train, validation or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset is already split\n",
    "if all(os.path.exists(d) and len(os.listdir(d)) > 0 for d in [TRAIN_DIR, VAL_DIR, TEST_DIR]):\n",
    "    print(\"Dataset already split.\")\n",
    "else:\n",
    "    labels = os.listdir(DATASET_DIR)\n",
    "\n",
    "    for label in labels:\n",
    "        label_path = os.path.join(DATASET_DIR, label)\n",
    "\n",
    "        os.makedirs(os.path.join(TRAIN_DIR, label), exist_ok=True)\n",
    "        os.makedirs(os.path.join(VAL_DIR, label), exist_ok=True)\n",
    "        os.makedirs(os.path.join(TEST_DIR, label), exist_ok=True)\n",
    "\n",
    "        images = os.listdir(label_path)\n",
    "        train_paths, temp_paths = train_test_split(images, train_size=TRAIN_SIZE)\n",
    "        val_paths, test_paths = train_test_split(temp_paths, train_size=(VAL_SIZE / (1 - TRAIN_SIZE)))\n",
    "\n",
    "        for image in train_paths:\n",
    "            shutil.copy(os.path.join(label_path, image), os.path.join(TRAIN_DIR, label, image))\n",
    "\n",
    "        for image in train_paths:\n",
    "            shutil.copy(os.path.join(label_path, image), os.path.join(VAL_DIR, label, image))\n",
    "\n",
    "        for image in test_paths:\n",
    "            shutil.copy(os.path.join(label_path, image), os.path.join(TEST_DIR, label, image))\n",
    "\n",
    "    print(f\"Dataset has been split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ML constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transforms (and augmentations) for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize images to 256x256\n",
    "    transforms.RandomResizedCrop(224),   # Random crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # Convert to tensor & scale to [0, 1]\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),  # Convert to tensor & scale to [0, 1]\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting directory paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_and_labels(*directories):\n",
    "    paths = []\n",
    "    labels = []\n",
    "\n",
    "    for directory in directories:\n",
    "        for idx, label in enumerate(CLASSES):\n",
    "            class_dir = os.path.join(directory, label)\n",
    "            for image in os.listdir(class_dir):\n",
    "                path = os.path.join(class_dir, image)\n",
    "\n",
    "                paths.append(path)\n",
    "                labels.append(idx)\n",
    "\n",
    "    return paths, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(utils.data.Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(directory, transform=None):\n",
    "    paths, labels = get_path_and_labels(directory)\n",
    "    return ImageDataset(paths, labels, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(dataset, shuffle=False):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, shuffle=shuffle, pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "    # Freeze feature extraction layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(train_loader):\n",
    "    all_labels = []\n",
    "\n",
    "    for _, labels in train_loader:\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)  # Convert list to NumPy array\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(all_labels), y=all_labels)\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    return class_weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, train_loader, val_loader, epochs=EPOCHS, early_stopping_patience=5, reduce_lr_patience=5, verbose=1, model_path=OUTPUT_DIR, model_name=\"model.pt\"):\n",
    "    start = time.time()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=get_class_weights(train_loader))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=reduce_lr_patience, factor=0.5)\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "    \n",
    "    if model_path is not None:\n",
    "        best_model_path = os.path.join(model_path, model_name)\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save initial model\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for phase, dataset in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "            model.train() if phase == \"train\" else model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            correct, total = 0, 0\n",
    "\n",
    "            for images, labels in dataset:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataset.dataset)\n",
    "            epoch_acc = correct / total\n",
    "\n",
    "            history[f\"{phase}_loss\"].append(epoch_loss)\n",
    "            history[f\"{phase}_accuracy\"].append(epoch_acc)\n",
    "\n",
    "            if phase == \"val\":\n",
    "                scheduler.step(epoch_loss)  # Reduce LR based on val_loss\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_acc = epoch_acc\n",
    "                    if model_path is not None:\n",
    "                        torch.save(model.state_dict(), best_model_path)\n",
    "                        \n",
    "                    epochs_no_improve = 0  # Reset early stopping counter\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                  f\"Train Loss: {history['train_loss'][-1]:.4f}, Train Accuracy: {history['train_accuracy'][-1]:.4f}, \"\n",
    "                  f\"Val Loss: {history['val_loss'][-1]:.4f}, Val Accuracy: {history['val_accuracy'][-1]:.4f}, \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']}\")\n",
    "        \n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            if verbose > 0:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs without improvement in val_loss.\")\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    if verbose > 0:\n",
    "        print(f\"Training completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "        print(f\"Best validation Loss: {best_loss:.4f} and it's respective accuracy: {best_acc:.4f}.\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(history, metric, path=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    epochs = np.arange(1, len(history[f\"train_{metric}\"]) + 1)\n",
    "\n",
    "    plt.plot(epochs, history[f\"train_{metric}\"], label=f\"train_{metric}\")\n",
    "    plt.plot(epochs, history[f\"val_{metric}\"], label=f\"val_{metric}\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    \n",
    "    if path is not None:\n",
    "        plt.savefig(os.path.join(path, f\"{metric}.png\"))\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_k_fold(k=5):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    X, y = get_path_and_labels(TRAIN_DIR, VAL_DIR)\n",
    "\n",
    "    for fold_no, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        start_time = time.time()\n",
    "\n",
    "        X_train, X_val = np.array(X)[train_index], np.array(X)[val_index]\n",
    "        y_train, y_val = np.array(y)[train_index], np.array(y)[val_index]\n",
    "\n",
    "        train_dataset = ImageDataset(X_train, y_train, transform=train_transform)\n",
    "        val_dataset = ImageDataset(X_val, y_val, transform=test_transform)\n",
    "\n",
    "        train_loader = prepare(train_dataset, shuffle=True)\n",
    "        val_loader = prepare(val_dataset)\n",
    "\n",
    "        model = create_model()\n",
    "\n",
    "        history = run_model(model, train_loader, val_loader, epochs=EPOCHS, early_stopping_patience=10, reduce_lr_patience=2, verbose=0, model_path=None)\n",
    "\n",
    "        accuracies.append(history[\"val_accuracy\"][-1] * 100)\n",
    "        losses.append(history[\"val_loss\"][-1])\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Fold {fold_no + 1}/{k} - Completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "\n",
    "    print(f\"K-Fold completed! Average validation accuracy: {np.mean(accuracies):.2f}%. Average validation loss: {np.mean(losses):.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training and evaluating the model using stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    test_model_k_fold(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(directory=TRAIN_DIR, transform=train_transform)\n",
    "val_dataset = create_dataset(directory=VAL_DIR, transform=test_transform)\n",
    "test_dataset = create_dataset(directory=TEST_DIR, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = prepare(train_dataset, shuffle=True)\n",
    "val_loader = prepare(val_dataset)\n",
    "test_loader = prepare(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = None\n",
    "model_dir = os.path.join(OUTPUT_DIR, \"model\")\n",
    "\n",
    "if TESTING:\n",
    "    model = create_model()\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    history = run_model(model, train_loader, val_loader, epochs=EPOCHS, early_stopping_patience=10, reduce_lr_patience=2, model_path=model_dir, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plotting accuracy and loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history and SHOW_VISUALIZATIONS:\n",
    "    plot_metric(history, \"accuracy\", model_dir)\n",
    "\n",
    "if history and SHOW_VISUALIZATIONS:\n",
    "    plot_metric(history, \"loss\", model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, MODEL_NAME), weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the true labels for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_labels(test_loader):\n",
    "    true_labels = []\n",
    "    for _, labels in test_loader:\n",
    "        true_labels.extend(labels.numpy())\n",
    "\n",
    "    return np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = get_true_labels(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute and plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(os.path.join(model_dir, \"conf_matrix.png\"), dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute and display the classification report (including precision, recall and f1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conclusion on Waste Classification Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The waste classification model demonstrates strong overall performance, achieving an accuracy of 92%. The weighted F1-score of 0.92 further confirms that the model is making balanced predictions across all classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Class-Specific Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cardboard shows exceptional performance with an F1-score of 0.96, indicating that it is rarely misclassified.  \n",
    "- Glass, metal, paper, and plastic** all have F1-scores above 0.90*, suggesting that the model is highly effective at distinguishing these materials.  \n",
    "- Trash has the lowest F1-score (0.83), indicating that it is the most challenging category for the model. This could be due to its more varied visual characteristics, leading to confusion with other classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Key Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The macro F1-score (0.91) and recall (0.91) show that the model is consistently accurate across different classes, rather than being biased towards dominant categories.  \n",
    "- The precision-recall balance is well maintained, meaning the model is not significantly over-predicting or under-predicting any particular class.  \n",
    "- The higher recall for \"trash\" (0.86) suggests that the model correctly identifies most actual trash items, though it may sometimes misclassify other items as trash.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Potential Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enhancing \"trash\" classification: Collecting more diverse samples or fine-tuning the model on this class could improve its performance.  \n",
    "- Addressing misclassifications: Analysing the confusion matrix could reveal patterns in misclassifications and guide further refinements.  \n",
    "- Model deployment considerations: Since the model performs well across classes, it could be suitable for real-world applications such as automated recycling systems or waste sorting robots.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Final Verdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is highly effective for waste classification, with only minor areas for improvement, particularly in distinguishing trash from other categories. It is well-suited for practical applications where high precision and recall are required for sustainable waste management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Next Steps: Pruning and Quantisation for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimise the model for deployment on mobile and edge devices, the following steps will be undertaken:  \n",
    "- **Pruning**: Reducing unnecessary parameters to make the model more efficient without significantly impacting accuracy.  \n",
    "- **Quantisation**: Converting the model to a lower precision format (8-bit integers) to reduce size and improve inference speed.  \n",
    "\n",
    "These techniques will ensure the model remains lightweight, efficient, and suitable for real-world deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning(model, amount=0.3):\n",
    "    # https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n",
    "    parameters_to_prune = []\n",
    "\n",
    "    # Add convolutional and linear layers to prune\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            parameters_to_prune.append((module, \"weight\"))\n",
    "\n",
    "    # Apply global unstructured pruning\n",
    "    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=amount)\n",
    "\n",
    "    # Make pruning permananet\n",
    "    for layer, name in parameters_to_prune:\n",
    "        prune.remove(layer, name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, MODEL_NAME), weights_only=True))\n",
    "pruned_model = apply_pruning(model, amount=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fine-tuning the pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, train_loader, epochs=5, model_path=OUTPUT_DIR, model_name=\"pruned_model.pt\"):\n",
    "    start = time.time()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=get_class_weights(train_loader))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    # Save fine-tuned model\n",
    "    if model_path is not None:\n",
    "        fine_tuned_model_path = os.path.join(model_path, model_name)\n",
    "        torch.save(model.state_dict(), fine_tuned_model_path)\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Fine-tuning completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_model(pruned_model, train_loader, epochs=5, model_path=model_dir, model_name=f\"pruned_{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantisation (Post-Training Dynamic Quantisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantisation(model):\n",
    "    # https://pytorch.org/docs/stable/quantization.html\n",
    "    quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, MODEL_NAME), weights_only=True))\n",
    "quantised_model = apply_quantisation(model)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(model_dir, f\"quantised_{MODEL_NAME}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pruning and Quantisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, MODEL_NAME), weights_only=True))\n",
    "\n",
    "pruned_model = apply_pruning(model, amount=0.3)\n",
    "\n",
    "fine_tune_model(pruned_model, train_loader, epochs=5, model_path=\"\", model_name=f\"temp_pruned_{MODEL_NAME}\")\n",
    "\n",
    "pruned_ft_model = create_model()\n",
    "pruned_ft_model.load_state_dict(torch.load(os.path.join(\"\", f\"temp_pruned_{MODEL_NAME}\"), weights_only=True))\n",
    "pruned_and_quantised_model = apply_quantisation(pruned_ft_model)\n",
    "\n",
    "# Delete the temporary pruned model\n",
    "os.remove(f\"temp_pruned_{MODEL_NAME}\")\n",
    "\n",
    "torch.save(quantised_model.state_dict(), os.path.join(model_dir, f\"pruned_and_quantised_{MODEL_NAME}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
